{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Uni-Facef - DS - Regressão Linear com Pyspark </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento de um modelo com Regressão Linear usando PySpark\n",
    "\n",
    "   O objetivo da Regressão Linear é prever os valores de uma variável dependente com base em resultados da variável independente. Então, da fórmula de regressão linear Y = ax + b, \"x\" é a variável independente (preditora) e y é a variável dependente (resposta), uma vez que \"y\" depende de \"x\". É um algoritmo de Aprendizado Supervisionado.\n",
    "\n",
    "Baseado no artigo: https://www.cetax.com.br/blog/tutorial-pyspark-e-mllib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv('Salary_Data.csv', header=False)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renomeia os campos do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumnRenamed(\"_c0\", \"YearsExperience\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"Salary\") \\\n",
    "    .select(\"Salary\", \"YearsExperience\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "# Função para modifica o tipo de uma lista de colunas\n",
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "\n",
    "# Lista das colunas para coversão\n",
    "columns = ['YearsExperience', 'Salary']\n",
    "\n",
    "\n",
    "# Converte as colunas para \"FloatType()\"\n",
    "df = convertColumn(df, columns, FloatType())\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"Salary\").count().sort(\"Salary\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O método \"describe()\"\n",
    "\n",
    "Faz a descrição do \"df\" baseado nas colunas, retornando uma contagem dos elementos, a média, desvio padrão, valores mínimo e máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Módulo DenseVector\n",
    "\n",
    "O uso do DenseVector é uma maneira otimizada de lidar com valores numéricos, acelerando o processamento realizado pelo Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define o RDD \"input_data\" para aplicar DenseVector \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Colocando os dados na mesma escala com método StantardScaler\n",
    "\n",
    "O algoritmo de Regressão Linear trabalha com distâncias euclidianas, ele realiza operações de distância entre pontos no plano cartesiano. Isso exige que os dados estejam na mesma escala.\n",
    "\n",
    "Com o StantardScaler, subtrai-se a média do valor \"x\" definido, e divide-se pela diferença (xmax – xmin). Desta forma, os dados estarão distribuidos ao longo de 0 na mesma escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "\n",
    "# Cria a instância do objeto\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit no DataFrame\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transforma em dataframe com a escala\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "print(type(scaled_df))\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(scaled_df.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slit em dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.75,.25],seed=1234)\n",
    "\n",
    "train_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instanciando o algoritmo e treinando o modelo (fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)\n",
    "\n",
    "type(linearModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fazendo a predição com os dados de Treino\n",
    "\n",
    "Criará a nova coluna \"prediction\" que dá para confrontar com a coluna \"label\" (variável resposta de treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coeficientes da equação\n",
    "\n",
    "Visto que a equação é \"Y = ax + b\"\n",
    "\n",
    "Onde \"coefficients\" é o valor de \"a\", e \"intercept\" é o valor de \"b\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients para o modelo\n",
    "linearModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept para o modelo\n",
    "linearModel.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Erro Quadrático Médio, em inglês MSE, neste caso sendo representado pela raiz quadrada deste valor(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the RMSE\n",
    "linearModel.summary.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Métrica de avaliação do modelo\n",
    "\n",
    "Extrai-se o R2, ou Coeficiente de Determinação, uma métrica estatística de proximidade de pontos e reta sobreposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearModel.summary.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentação\n",
    "\n",
    "https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
