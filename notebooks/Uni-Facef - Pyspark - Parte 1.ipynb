{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Uni-Facef - Pyspark - Parte 1 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instalando o pacote PySpark em nosso \"Cluster de uma Máquina Só rs\"\n",
    "\n",
    "Lembre-se que para utilização do pacote PySpark é necessário fazer a instalação do Apache Spark na máquina "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação um sessão no Spark para no Aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('ReceitasGov2020') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "type(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copiando uma arquivo para o Google Colab Lab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação de um dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um DataFrame é um conjunto de dados organizado em colunas nomeadas. É conceitualmente equivalente a uma tabela em um banco de dados relacional ou um quadro de dados em R / Python, mas com otimizações mais ricas sob o capô. Os DataFrames podem ser construídos a partir de uma ampla variedade de fontes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .csv('Receitas2020.csv', header=True)\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listando os métodos do objeto DataFrame\n",
    "dir(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conta a quantidade de registros do Dataframe\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra a descrição do Método\n",
    "help(df.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra a Schema Inferido para o Dataframa\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra os primeiros registros do dataset (default 20)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para os novos nomes\n",
    "novos_nomes = [\n",
    "    \"cod_orgao_superior\", \"nome_orgao_superior\", \"cod_orgao\",\n",
    "    \"nome_orgao\", \"cod_unidade_gestora\", \"nome_unidade_gestora\",\n",
    "    \"categoria_economica\", \"origem_receita\", \"especie_receita\",\n",
    "    \"detalhamento\", \"vr_previsto_atualiz\", \"vr_lancado\",\n",
    "    \"vr_realizado\", \"percent_realizado\", \"data_lancamento\",\n",
    "    \"ano_exercicio\"\n",
    "]\n",
    "\n",
    "for i, coluna in enumerate(df.columns):\n",
    "    print(i, coluna)\n",
    "    \n",
    "print(\"\\n1 \" + novos_nomes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicando alteração no nome dos campos através de funções do módulo \"pyspark.sql.functions\"\n",
    "\n",
    "- col() - Para selecionar a coluna que vamos utilizar\n",
    "- alias() - Para modificar o nome do campo\n",
    "\n",
    "Obs: A função .upper() e enumerate() são funções do Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "df = df.select(\n",
    "    [sf.col(coluna).alias(novos_nomes[i].upper()) for i, coluna in enumerate(df.columns)]\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando schema com o módulo \"pyspark.sql.types\"\n",
    "\n",
    "É possível definir nome e tipo das colunas do Dataframe, se é opcional ou não, no momento da leitura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as st\n",
    "\n",
    "SCHEMA = st.StructType([\n",
    "    st.StructField(\"cod_orgao_superior\", st.LongType(), True),\n",
    "    st.StructField(\"nome_orgao_superior\", st.StringType(), True),\n",
    "    st.StructField(\"cod_orgao\", st.LongType(), True),\n",
    "    st.StructField(\"nome_orgao\", st.StringType(), True),\n",
    "    st.StructField(\"cod_unidade_gestora\", st.LongType(), True),\n",
    "    st.StructField(\"nome_unidade_gestora\", st.StringType(), True),\n",
    "    st.StructField(\"categoria_economica\", st.StringType(), True),\n",
    "    st.StructField(\"origem_receita\", st.StringType(), True),\n",
    "    st.StructField(\"especie_receita\", st.StringType(), True),\n",
    "    st.StructField(\"detalhamento\", st.StringType(), True),\n",
    "    st.StructField(\"vr_previsto_atualiz\", st.DecimalType(10, 2), True),\n",
    "    st.StructField(\"vr_lancado\", st.DecimalType(10, 2), True),\n",
    "    st.StructField(\"vr_realizado\", st.DecimalType(10, 2), True),\n",
    "    st.StructField(\"percent_realizado\", st.DecimalType(10, 2), True),\n",
    "    st.StructField(\"data_lancamento\", st.StringType(), True),\n",
    "    st.StructField(\"ano_exercicio\", st.IntegerType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nova leitura do Dataset resolvendo problema do Schema e Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"encoding\", \"ISO-8859-1\") \\\n",
    "    .csv('Receitas2020.csv', header=True, schema=SCHEMA)\n",
    "\n",
    "# O parâmetro truncate define se mostra tudo conteudo do campo ou não\n",
    "df.show(5, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformando valores através do modulo \"pyspark.sql.functions\"\n",
    "\n",
    "- to_timestamp(): Converte o valor string em timestamp, passando o formato atual da string\n",
    "- cast(): Converte o valor para o type desejado conforme \"pyspark.sql.types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st\n",
    "\n",
    "df = df \\\n",
    "    .withColumn(\"data_lancamento\", sf.to_timestamp(sf.col(\"data_lancamento\"),\"dd/MM/yyyy\")) \\\n",
    "    .withColumn(\"data_lancamento\", sf.col(\"data_lancamento\").cast(st.DateType())) \\\n",
    "    #.withColumn(\"data_lancamento\", sf.col(\"data_lancamento\").cast(\"date\")) \\\n",
    "\n",
    "df.select(\"data_lancamento\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando o conteúdo do campo - Valores acentuados\n",
    "df.select(\"detalhamento\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando uma UDF (User-defined function)\n",
    "\n",
    "Criando uma função simples em Python é possível registrá-la como um UDFs e trabalhar da mesma forma como um função do pacote \"pyspark.sql.functions\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import normalize, category\n",
    "\n",
    "def remove_acento(texto):\n",
    "    \"\"\"\n",
    "    Remove acentos de um string:\n",
    "    Essa função utiliza os métodos \"normalize\" e \"category\" do módulo \"unicodedata\"\n",
    "    Também utiliza \"list comprehension\" para iterar nos caracteres da string\n",
    "    \"\"\"\n",
    "    if not texto:\n",
    "        return None\n",
    "    \n",
    "    return ''.join((c for c in normalize('NFD', texto) if category(c) != 'Mn'))\n",
    "\n",
    "# Registra a UDF\n",
    "remove_acento_udf = sf.udf(remove_acento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realizando transformações nos campos desejados, usando a UDF \"remove_acento_udf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumn(\"nome_orgao_superior\", remove_acento_udf(\"nome_orgao_superior\")) \\\n",
    "    .withColumn(\"nome_orgao\", remove_acento_udf(\"nome_orgao\")) \\\n",
    "    .withColumn(\"nome_unidade_gestora\", remove_acento_udf(\"nome_unidade_gestora\")) \\\n",
    "    .withColumn(\"categoria_economica\", remove_acento_udf(\"categoria_economica\")) \\\n",
    "    .withColumn(\"origem_receita\", remove_acento_udf(\"origem_receita\")) \\\n",
    "    .withColumn(\"especie_receita\", remove_acento_udf(\"especie_receita\")) \\\n",
    "    .withColumn(\"detalhamento\", remove_acento_udf(\"detalhamento\"))\n",
    "\n",
    "df.select(\"detalhamento\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve em formato parquet\n",
    "df.repartition(2).write.parquet(\n",
    "    \"Receitas2020\", \n",
    "    mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso esteja utilizando o Google Colab, faça o Download dos arquivos parquets para utilização no script \"Uni-Facef - Pyspark - Parte 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /content/Receitas2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIM\n",
    "###### Documentação: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
