{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Uni-Facef - Pyspark - Parte 4 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notabook vamos descobrir como funciona o método \"readStream()\" do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pyspark.sql.types as st\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('readStream Example') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.readStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "root\n",
    " |-- id_cliente: string (nullable = true)\n",
    " |-- nome_cliente: string (nullable = true)\n",
    " |-- end_logradouro: string (nullable = true)\n",
    " |-- numero: string (nullable = true)\n",
    " |-- cidade: string (nullable = true)\n",
    " |-- dt_ult_atualizacao: timestamp (nullable = true)\n",
    " |-- datalog: string (nullable = false)\n",
    "\"\"\"\n",
    "\n",
    "SCHEMA = st.StructType([\n",
    "    st.StructField(\"id_cliente\", st.StringType(), True),\n",
    "    st.StructField(\"nome_cliente\", st.StringType(), True),\n",
    "    st.StructField(\"end_logradouro\", st.StringType(), True),\n",
    "    st.StructField(\"numero\", st.StringType(), True),\n",
    "    st.StructField(\"cidade\", st.StringType(), True),\n",
    "    st.StructField(\"dt_ult_atualizacao\", st.TimestampType(), True),\n",
    "    st.StructField(\"datalog\", st.StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cliente = spark.readStream \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .csv('cliente_csv', header=True, schema=SCHEMA)\n",
    "\n",
    "df_cliente.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos criar um processo incremental\n",
    "\n",
    "Para isso vamos precisar definir uma variável com o caminho/pasta do novo dataset e o caminho/pasta dos metadados do checkpoint do processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminho da nossa pasta de trabalho\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PATH = \"/home/al_morais/Documentos/UniFacef/notebooks\"\n",
    "\n",
    "CHECKPOINT = HOME_PATH + \"/checkpoint/cliente_raw/\"\n",
    "DATASET_PATH = HOME_PATH + \"/cliente_raw/\"\n",
    "\n",
    "print(CHECKPOINT)\n",
    "print(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos ler os arquivos de origem e criar um novo dataset. \n",
    "\n",
    "Com o \"readStream\" e \"writeStream\" o processo garante a integridade de processos incrementais, pois os arquivos já processados não serão processados novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do Dataset de origem\n",
    "df_cliente = spark.readStream \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .csv('cliente_csv', header=False, schema=SCHEMA)\n",
    "\n",
    "# Transformação nos dados\n",
    "df_cliente = df_cliente \\\n",
    "    .withColumn(\"datalog\", sf.col(\"dt_ult_atualizacao\").cast(\"date\"))\n",
    "\n",
    "# Escrita \n",
    "df_cliente.writeStream.trigger(once=True) \\\n",
    "    .start(\n",
    "        path=DATASET_PATH,\n",
    "        format=\"parquet\",\n",
    "        checkpointLocation=CHECKPOINT,\n",
    "        partitionBy=[\"datalog\"]).awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pasta do dataset contem agora uma pasta de metadados chamada \"_spark_metadata\" ao invés de \"_SUCCESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /home/al_morais/Documentos/UniFacef/notebooks/cliente_raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasta criada para armazenar os metadados do checkpoint deste dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /home/al_morais/Documentos/UniFacef/notebooks/checkpoint/cliente_raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando a leitura \"Full\" do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novo_cliente_raw = spark.read.parquet('cliente_raw')\n",
    "\n",
    "novo_cliente_raw.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIM\n",
    "###### Documentação: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
