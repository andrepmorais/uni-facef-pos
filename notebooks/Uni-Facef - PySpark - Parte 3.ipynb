{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Uni-Facef - PySpark Parte 3 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook simula um response de uma API e que recebe uma mensagem no padrão Json. Com isso, esse dataset receberá o tratamento necessário para para fazer o \"parsing\" da mensagem e a normalização estruturada da informação "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Parsing Json') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\"body\": [\n",
    "    {\"id\": \"1000\",\n",
    "     \"nome\": \"André\",\n",
    "     \"endereco\": {\"logradouro\": \"Rua Joaquim Cruz\",\n",
    "                  \"numero\": \"534\",\n",
    "                  \"cidade\": \"Franca\"},\n",
    "     \"tags\": [\"Grupo1\", \"Grupo2\"],\n",
    "     \"dt_ult_atualizacao\":\"2020-08-09 00:00:00\"},\n",
    "    {\"id\": \"1001\",\n",
    "     \"nome\": \"Felipe\",\n",
    "     \"endereco\": {\"logradouro\": \"Av Nova Lua\",\n",
    "                  \"numero\": \"1093\",\n",
    "                  \"cidade\": \"São Paulo\"},\n",
    "     \"tags\": [\"Grupo1\", \"Grupo3\"],\n",
    "     \"dt_ult_atualizacao\":\"2020-08-11 00:00:00\"},\n",
    "    {\"id\": \"1002\",\n",
    "     \"nome\": \"Maria\",\n",
    "     \"endereco\": {\"logradouro\": \"Rua J\",\n",
    "                  \"numero\": \"10\",\n",
    "                  \"cidade\": \"Araraquara\"},\n",
    "     \"tags\": [\"Grupo3\"],\n",
    "     \"dt_ult_atualizacao\":\"2020-08-13 00:00:00\"}\n",
    "]}\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(response))\n",
    "print(type(response['body']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(response['body'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O DataFrame não reconhece a estrutura no segundo nível do endereço\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criando o mesmo DataFrame apartir de um RDD usando a função parallelize() e map()\n",
    "\n",
    "- parallelize() - É uma função em SparkContext e é usado para criar um RDD de uma coleção de lista.\n",
    "\n",
    "- map() - É o método do RDD da qual recebe uma função como parâmetro e passa por todos os elementos do RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(response['body']).map(lambda x: json.dumps(x))\n",
    "df = spark.read.json(rdd)\n",
    "\n",
    "print(df.printSchema())\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizando as informações de primeiro nível do documento\n",
    "\n",
    "O campo endereço é um objeto do tipo \"struct\", onde armazena informações num segundo nível. Porém nesse caso possui uma relação 1:1 com os campos do primeiro. Para acessar a informação: \"campo_1_nivel.campo_2_nivel\"\n",
    "\n",
    "``` \n",
    "{'id': '1000',\n",
    " 'nome': 'André',\n",
    " 'endereco': {'logradouro': 'Rua Joaquim Cruz',\n",
    "              'numero': '534',\n",
    "              'cidade': 'Franca'},\n",
    " 'tags': ['Grupo1', 'Grupo2'],\n",
    " 'dt_ult_atualizacao': '2020-08-08 00:00:00'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente = df.select(\n",
    "    sf.col(\"id\").alias(\"id_cliente\"),\n",
    "    sf.col(\"nome\").alias(\"nome_cliente\"),\n",
    "    sf.col(\"endereco.logradouro\").alias(\"end_logradouro\"),\n",
    "    sf.col(\"endereco.numero\").alias(\"numero\"),\n",
    "    sf.col(\"endereco.cidade\").alias(\"cidade\"),\n",
    "    sf.col(\"dt_ult_atualizacao\").alias(\"dt_ult_atualizacao\")) \\\n",
    "    .withColumn(\"dt_ult_atualizacao\", sf.col(\"dt_ult_atualizacao\").cast('timestamp'))\n",
    "\n",
    "cliente.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizando as informações de tags que possui uma relação 1:N\n",
    "\n",
    "```\n",
    "'tags': ['Grupo1', 'Grupo2']\n",
    "```\n",
    "Nesse caso é preciso normalizar estruturando as informações de \"tag\" em um novo dataset relacionável com o primeiro através da chave \"id_cliente\". usando a funções \"explode()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente_tag = df.select(\n",
    "    sf.col(\"id\").alias(\"id_cliente\"),\n",
    "    sf.explode(\"tags\").alias(\"tag\"))\n",
    "\n",
    "cliente_tag.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtra os clientes do \"Grupo3\" \n",
    "\n",
    "- Faz o filtro no dataframe \"cliente_tag\" através do método \"filter()\"\n",
    "- Faz join com o dataframe \"cliente\" através do método \"join()\"\n",
    "- Renomea a coluna \"tag\" usando o método \"withColumnRenamed()\"\n",
    "- Cria um novo campo chamado \"endereco_completo\" usando o método \"withColumn\"\n",
    "    - Para concatenação é utilizada a função \"concat\" \n",
    "    - Para concatenar valores \"fixos\" é utilizada a função \"lit()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_grupo3 = cliente_tag \\\n",
    "    .filter(cliente_tag.tag == \"Grupo3\") \\\n",
    "    .join(cliente, 'id_cliente', 'inner') \\\n",
    "    .withColumnRenamed(\"tag\", \"tag_cliente\") \\\n",
    "    .withColumn(\n",
    "        \"endereco_completo\", \n",
    "        sf.concat(\"end_logradouro\", sf.lit(\", \"), \"numero\"))\n",
    "\n",
    "\n",
    "cli_grupo3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gravando um dataset com mode append e partitionBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "cliente = cliente \\\n",
    "    .withColumn(\"datalog\", sf.lit(str(datetime.utcnow().date())))\n",
    "\n",
    "cliente.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve em formato parquet\n",
    "cliente.write.parquet(\n",
    "    \"cliente\", \n",
    "    mode=\"append\", \n",
    "    partitionBy=[\"datalog\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /content/cliente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /content/cliente/datalog=2022-09-24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gravar o dataset em CSV\n",
    "Vamos gravar o dataset em CSV simulando um Job Sqoop fazendo uma ingestão incremental de uma tabela de um banco relacional.\n",
    "A data da ultima atualização foi mudada para simular data/hora diferentes para utilizarmos no script \"Uni-Facef - PySpark - Parte 4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliente = cliente \\\n",
    "    .withColumn(\"dt_ult_atualizacao\", sf.lit(str(datetime.utcnow())).cast(\"timestamp\"))\n",
    "\n",
    "cliente.printSchema()\n",
    "cliente.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escreve em formato CSV como Append\n",
    "cliente.repartition(1).write \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .csv(\"cliente_csv\", header=True, mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /content/cliente_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o dataset gerado em CSV\n",
    "df_show = spark.read \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .csv('cliente_csv', header=True)\n",
    "\n",
    "df_show.orderBy(\"id_cliente\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIM\n",
    "###### Documentação: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
