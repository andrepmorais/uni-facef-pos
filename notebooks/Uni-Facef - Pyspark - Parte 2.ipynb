{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Uni-Facef - Pyspark - Parte 2 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neste notebook vamos fazer algumas transformações e agregações no Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando os módulos necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Agregação ReceitasGov') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caso esteja utilizando o Google Colab, crie a pasta \"Receitas2020\" e faça o upload dos arquivos parquet da parte 1 para dentro da pasta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /content/Receitas2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo o \"Dataset\" Receitas2020 em parquet. \n",
    "\n",
    "Repare que estamos fazendo a leitura em uma pasta que pode ter diversos arquivos com a mesma estrutura e que compreende à mesma informação que pode estar chegando no repositório de forma incremental\n",
    "\n",
    "O \"parquet\" é um formato tipado e colunar da qual o Spark trabalha com uma performance muito boa. Isso faz com que ele sejam uma excelente opção para trabalhar as \"zonas\" estruturadas em um Datalake . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receitas = spark.read.parquet('Receitas2020')\n",
    "\n",
    "df_receitas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receitas.show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receitas.select(\"origem_receita\").distinct().show(truncate=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O médoto \"groupBy()\" do PySpark Dataframe retorna o GroupedDataobjeto. Segue  as principais funções contidas nesse objeto:\n",
    "\n",
    "- count() - Retorna a contagem de linhas para cada grupo.\n",
    "- mean()  - Retorna a média dos valores de cada grupo.\n",
    "- max()   - Retorna o máximo de valores para cada grupo.\n",
    "- min()   - Retorna o mínimo de valores para cada grupo.\n",
    "- sum()   - Retorna o total de valores para cada grupo.\n",
    "- avg()   - Retorna a média dos valores de cada grupo.\n",
    "\n",
    "\n",
    "- agg()   - A função agg() é utilizada para calcular mais de uma valor agregado por vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receitas.groupBy(\"origem_receita\") \\\n",
    "    .sum(\"vr_realizado\") \\\n",
    "    .withColumnRenamed(\"sum(vr_realizado)\", \"sum_vr_realizado\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crindo um novo Dataframe \"df_origem_agg\", calculando mais um valor agregado por vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origem_agg = df_receitas \\\n",
    "    .filter(\"origem_receita is not null\") \\\n",
    "    .groupBy(\"origem_receita\") \\\n",
    "    .agg(sf.sum(\"vr_realizado\").alias(\"sum_vr_realizado\"),\n",
    "         sf.avg(\"vr_realizado\").alias(\"avg_vr_realizado\"),\n",
    "         sf.max(\"vr_realizado\").alias(\"max_vr_realizado\")) \\\n",
    "    .withColumn(\"avg_vr_realizado\", sf.round(\"avg_vr_realizado\", 2))\n",
    "\n",
    "print(df_origem_agg.count())\n",
    "\n",
    "df_origem_agg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trabalhando com sintaxe SQL no Spark através do método \"createOrReplaceTempView()\" do objeto Dataframe\n",
    "\n",
    "- createOrReplaceTempView() - Cria uma nova visualização temporária usando SparkDataFrame na Sessão do Spark. Se já existir uma visão temporária com o mesmo nome, substitui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_receitas.createOrReplaceTempView(\"v_receitas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct data_lancamento\n",
    "      from v_receitas\n",
    "    \"\"\")\n",
    "\n",
    "df_teste.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos construir querys dimâmicas passando variável e fazendo Interpolação de string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável\n",
    "filtro = \"origem_receita == 'Contribuicoes' and data_lancamento == '2020-01-21'\"\n",
    "\n",
    "query = f\"\"\"\n",
    "    select *\n",
    "      from v_receitas\n",
    "     where {filtro}\n",
    "\"\"\"\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_origem_agg = \"\"\"\n",
    "    select origem_receita, \n",
    "           sum(vr_realizado) as sum_vr_realizado,\n",
    "           avg(vr_realizado) as avg_vr_realizado,\n",
    "           max(vr_realizado) as max_vr_realizado\n",
    "      from v_receitas\n",
    "     where origem_receita is not null\n",
    "     group by origem_receita\n",
    "\"\"\"\n",
    "\n",
    "df_origem_agg2 = spark.sql(query_origem_agg) \\\n",
    "    .withColumn(\"avg_vr_realizado\", sf.round(\"avg_vr_realizado\", 2))\n",
    "\n",
    "df_origem_agg2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIM\n",
    "###### Documentação: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
